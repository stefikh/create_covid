# В этой папке вы можете увидеть 6 основных частей нашего кода, расскажем о каждой подробнее.

**1_find_news_with_covid.py** Здесь мы из всех данных отобираем только те, где содержится информация про распространение коронавируса. Для этого мы воспользовались регулярными выражениями. Мы записали всё в словарь формата {дата: новости} и сложили полученный словарь про распространение коронавируса в отдельный файл, который назвали *Covid_dict.json*. 

**2_find_locations.py** Мы нашли библиотеку, которая называется Natasha. Эта библиотека может извлекать именованные сущности, что нам нужно в первую очередь, а также лемматизировать слова, что нам понадобится в дальнейшем. Если соответственно у слова был тип LOC, то мы его добавляли в список, который располагается в начале каждого сообщения. Таким образом у нас получился файл *Country_and_message.json*, где к каждой дате в начале идёт список стран, а потом сообщение/новость.

**3_translate_and_coordinates.py** В этой части кода мы вновь проходимся по файлу. Причём по одному из четырёх, на который мы разделили наш большой предыдущий файл *Country_and_message.json*, его перед этим мы чистили вручную от неподходящих нам новостей. Теперь мы приводили все названия стран в начальную форму (например, России -> Россия). Нам это нужно для того, чтобы перевести эти страны на английский, так как библиотека plotly, которую мы используем для создания карты, поддерживает только английский. После перевода мы удалили повторы в локациях. (Множить сущности не к чему) В этой же части кода вы можете увидеть, что с помощью geopandas мы присваиваем координаты каждой из локаций, чтобы потом записать их в датасет и отметить на карте. Все результаты записывались в промежуточные файлы.

**4_dynamic_good_or_bad.py** После того, как каждой стране были присвоены координаты, было необходимо изучить динамику распространения вируса, пройдясь по тексту новости регулярными выражениями, присваивая каждой стране 1 ("+") или 0 ("-") в зависимости от содержания. Полученные данные были записаны в *Country_and_coord_and_dynFULL.json*. К сожалению, не все удалось охватить этими регулярными выражениями, некоторые новости были пропущены, их мы доразмечали вручную.

**5_delete_text.py** К тому моменту работы мы уже взяли из самого текста сообщений всё, что могли, поэтому следующим шагом стало удаление текста сообщений из словаря.

**6_create_data_and_make_map.py** Перед тем как создавать датафрейм, нужно было создать словарь, в котором динамика у каждой локации была бы накопительной, иначе не получилось бы репрезентативных данных. Мы создали новый словарь, в котором ключами были даты, а значениями - словари по каждой локации на этот момент. Каждой локации соответствовала накопленная ею динамика: встретив сообщение с динамикой 0 ("-") мы отнимали единицу, с динамикой 1 ("+") - соответственно прибавляли. Для координат мы создали отдельный словарь.
Чтобы на основе датафрейма получилась анимация, недостаточно было просто иметь в нем строками даты, а столбцами - страны. Поэтому датафрейм был создан такого вида: страны повторялись в строках столько раз, сколько было дат. В столбце "динамика" для каждой страны на каждый момент времени было соответствующее число. Последним шагом мы добавили каждой стране ее координаты. 
Функция плотли экспресс, которую мы решили использовать — гео-скаттер. Она в первую очередь требует в качестве аргументов:
1) фреймы анимации, в нашем случае это столбец "дата"
2) ширину и долготу - соответствующие столбцы в датафрейме
И наконец, самое главное, нужно показать саму динамику. Мы решили это сделать с помощью тепловой шкалы, которой окрашивали точки на карте. Параметру "цвет" мы присвоили значения столбца "Динамика". Мы настроили шкалу так, чтобы она была чувствительна к изменениям, а также настроили некоторые внешние параметры карты (скорость анимации, цвета земли, цвета границ и так далее)

Небольшая ремарка. До того, как очищать руками файл от ненужных новостей, которые не относились к нашей теме мы хотели это автоматизировать. В файле **code_part_tried_to_automise.py** вы можете увидеть нашу попытку написать код, который может сделать что-то подобное. К сожалению, он удалял слишком много данных, поэтому мы выбрали тот вариант, где мы вручную подготавливаем данные.
